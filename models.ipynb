{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "953f94a6-4a5b-44b0-ad84-7b033f0bca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import getsizeof\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea826e78-3f79-4afa-9306-2683f430c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "041738f2-119e-4d6a-87bb-59c95d9fdee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"new_train.csv\"\n",
    "image_path = \"new_train\"\n",
    "test_path = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0aa7a0c-e669-4ccc-848d-0f81544bbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    def __init__(self, name, model):\n",
    "        self.name = name\n",
    "        self.model = model\n",
    "\n",
    "    def train(self, train_data, val_data, y_train, y_val, batch_size=1000):\n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            start = i\n",
    "            end = min(i + batch_size, len(train_data))\n",
    "            print(f'Training {self.name} model on images {start} to {end}...')\n",
    "    \n",
    "            train_subset = train_data[start:end]\n",
    "            val_subset = val_data[start:end]\n",
    "\n",
    "            checkpoint_path = f\"{self.name}_checkpoint.h5\"\n",
    "            checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "            \n",
    "            self.model.fit(train_subset, y_train, epochs=50, batch_size=16, \n",
    "                           validation_data=(val_subset, y_val), \n",
    "                           callbacks=[checkpoint], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b66f7740-50fc-43e7-9d89-8890d39ca504",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelling:\n",
    "    def __init__(self, train_path, test_path, image_path):\n",
    "        self.train_data = pd.read_csv(train_path)\n",
    "        self.test_data = pd.read_csv(test_path) \n",
    "        self.image_path = image_path\n",
    "        self.image_amount = len(os.listdir(image_path))\n",
    "        \n",
    "        self.X_train_csv, self.X_val_csv, self.X_train_img, self.X_val_img, self.y_train, self.y_val = None, None, None, None, None, None\n",
    "        self.X_test = None\n",
    "\n",
    "        self.checkpoint_path = None\n",
    "        self.checkpoint = None\n",
    "\n",
    "        self.models = {'NN': \n",
    "                           {'function': self.create_NN, \n",
    "                            'model': None,\n",
    "                            'data': None,\n",
    "                            'history': pd.DataFrame({'loss': [], 'val_loss': []})},  \n",
    "                       'CNN': \n",
    "                           {'function': self.create_CNN, \n",
    "                            'model': None,\n",
    "                            'data': None,\n",
    "                            'history': pd.DataFrame({'loss': [], 'val_loss': []})}, \n",
    "                       'Transfer': \n",
    "                           {'function': self.create_Transfer, \n",
    "                            'model': None,\n",
    "                            'data': None,\n",
    "                            'history': pd.DataFrame({'loss': [], 'val_loss': []})}, \n",
    "                       'MultiModal': \n",
    "                           {'function': self.create_MultiModal, \n",
    "                            'model': None,\n",
    "                            'data': None,\n",
    "                            'history': pd.DataFrame({'loss': [], 'val_loss': []})}}\n",
    "\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    def prepare_data(self, start, end):\n",
    "        amount_of_images = end - start\n",
    "        scaler = StandardScaler() \n",
    "        \n",
    "        X = self.train_data.drop(['Price'], axis=1)\n",
    "        y = self.train_data['Price']\n",
    "        \n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        images = np.empty((amount_of_images, 512, 512, 3), dtype=np.uint8)\n",
    "        for i, image in enumerate(os.listdir(self.image_path)[start:end]):\n",
    "            image = cv.imread(f'{self.image_path}/{image}')\n",
    "            images[i, ...] = image\n",
    "\n",
    "        self.X_train_csv, self.X_val_csv, self.X_train_img, self.X_val_img, self.y_train, self.y_val = \\\n",
    "        train_test_split(X_scaled[start:end], images, y[start:end], test_size=0.2, random_state=42)\n",
    "\n",
    "        self.models['NN']['data'] = (self.X_train_csv, self.X_val_csv)\n",
    "        self.models['CNN']['data'] = (self.X_train_img, self.X_val_img)\n",
    "        self.models['Transfer']['data'] = ([self.X_train_img,  self.X_train_csv], [self.X_val_img, self.X_val_csv])\n",
    "        self.models['MultiModal']['data'] = ([self.X_train_img,  self.X_train_csv], [self.X_val_img, self.X_val_csv])\n",
    "        \n",
    "        #Standariseren \n",
    "        self.X_test = scaler.transform(self.test_data)\n",
    "\n",
    "    def create_NN(self):\n",
    "        self.models['NN']['model'] = keras.Sequential()\n",
    "\n",
    "        self.models['NN']['model'].add(layers.Input(shape=(6)))\n",
    "        self.models['NN']['model'].add(layers.Dense(256, activation='relu'))\n",
    "        self.models['NN']['model'].add(layers.Dropout(0.5))\n",
    "        self.models['NN']['model'].add(layers.Dense(128, activation='relu'))\n",
    "        self.models['NN']['model'].add(layers.Dropout(0.3))\n",
    "        self.models['NN']['model'].add(layers.Dense(64, activation='relu'))\n",
    "        self.models['NN']['model'].add(layers.Dense(1))\n",
    "        \n",
    "        self.models['NN']['model'].compile(optimizer=self.optimizer, loss='mean_absolute_percentage_error')\n",
    "\n",
    "    def create_CNN(self):\n",
    "        pass\n",
    "\n",
    "    def create_Transfer(self):\n",
    "        pass\n",
    "        \n",
    "    def create_MultiModal(self):\n",
    "        img_input = layers.Input(shape=(512, 512, 3))\n",
    "        csv_input = layers.Input(shape=(6))\n",
    "        \n",
    "        # define layers for image data \n",
    "        x_img = layers.experimental.preprocessing.Rescaling(1./255)(img_input)\n",
    "        x_img = layers.Conv2D(16, 3, padding='same', activation='relu')(x_img)\n",
    "        x_img = layers.MaxPooling2D()(x_img)\n",
    "        x_img = layers.Conv2D(32, 3, padding='same', activation='relu')(x_img)\n",
    "        x_img = layers.MaxPooling2D()(x_img)\n",
    "        x_img = layers.Conv2D(64, 3, padding='same', activation='relu')(x_img)\n",
    "        x_img = layers.MaxPooling2D()(x_img)\n",
    "        x_img = layers.Flatten()(x_img)\n",
    "        \n",
    "        # define layers for csv data\n",
    "        x_csv = layers.Flatten()(csv_input)\n",
    "        x_csv = layers.Dense(16, activation='relu')(x_csv)\n",
    "        x_csv = layers.Dense(32, activation='relu')(x_csv)\n",
    "        x_csv = layers.Dense(64, activation='relu')(x_csv)\n",
    "        \n",
    "        # merge layers\n",
    "        x = layers.concatenate([x_img, x_csv])\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        output = layers.Dense(1)(x)\n",
    "        \n",
    "        # make model with 2 inputs and 1 output\n",
    "        self.models['MultiModal']['model'] = tf.keras.models.Model(inputs=[img_input, csv_input], outputs=output)\n",
    "\n",
    "        self.models['MultiModal']['model'].compile(optimizer=self.optimizer, loss='mean_absolute_percentage_error')\n",
    "        \n",
    "    def train_model(self, model, batch_size=1000):  \n",
    "        try:\n",
    "            self.models[model]['function']()\n",
    "            print('model')\n",
    "        except:\n",
    "            print('This model does not exist!')\n",
    "            \n",
    "        for i in range(0, 200, batch_size):\n",
    "            self.models[model]['function']()\n",
    "            start = i\n",
    "            end = min(i + batch_size, self.image_amount)\n",
    "            print(f'Training {model} on images {start} to {end}...')\n",
    "    \n",
    "            tf.keras.backend.clear_session()\n",
    "            \n",
    "            self.prepare_data(start, end)\n",
    "            \n",
    "            self.checkpoint_path = f\"{model}_checkpoint.h5\"\n",
    "            self.checkpoint = ModelCheckpoint(self.checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "            print(self.models[model]['model'])\n",
    "            \n",
    "            self.models[model]['model'].fit(self.models[model]['data'][0], self.y_train, epochs=1, batch_size=16, \\\n",
    "                                            validation_data=(self.models[model]['data'][1], self.y_val), \\\n",
    "                                            callbacks=[self.early_stopping, self.checkpoint], verbose=0)\n",
    "            gc.collect()\n",
    "\n",
    "            history = pd.DataFrame(self.models[model]['model'].history.history)\n",
    "            self.models[model]['history'] = pd.concat([self.models[model]['history'], history], ignore_index=True)\n",
    "    def evaluate_models(self):\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        for model in self.models.keys():\n",
    "            plt.tight_layout()\n",
    "            ax[0].plot(self.models[model]['history']['loss'], label=f'{model} Train Loss')\n",
    "            ax[0].set_xlabel('Epochs')\n",
    "            ax[0].set_ylabel('Mean Absolute Percentage Error (mape)')\n",
    "            ax[0].legend()\n",
    "            \n",
    "            ax[1].plot(self.models[model]['history']['val_loss'], label=f'{model} Val Loss')\n",
    "            ax[1].set_xlabel('Epochs')\n",
    "            ax[1].set_ylabel('Mean Absolute Percentage Error (mape)')\n",
    "            ax[1].legend()\n",
    "        else:\n",
    "            print('No models trained...')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7720ddd-4f38-4ddf-88ad-aa1e76f55d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling = Modelling(train_path, test_path, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62d1f2cc-b81d-4bd6-abc3-d308ed8e7ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "Training NN on images 0 to 100...\n",
      "<keras.engine.sequential.Sequential object at 0x0000021330FFAA10>\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 99.99998, saving model to NN_checkpoint.h5\n",
      "Training NN on images 100 to 200...\n",
      "<keras.engine.sequential.Sequential object at 0x000002138F0B27D0>\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 99.99968, saving model to NN_checkpoint.h5\n",
      "model\n",
      "Training MultiModal on images 0 to 100...\n",
      "<keras.engine.functional.Functional object at 0x0000021328819210>\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 99.85835, saving model to MultiModal_checkpoint.h5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to synchronously create dataset (name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m modelling\u001b[38;5;241m.\u001b[39mtrain_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodelling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMultiModal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 134\u001b[0m, in \u001b[0;36mModelling.train_model\u001b[1;34m(self, model, batch_size)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_path, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[model][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m    139\u001b[0m history \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[model][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cuda\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cuda\\lib\\site-packages\\h5py\\_hl\\group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmake_new_dset(group, shape, dtype, data, name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cuda\\lib\\site-packages\\h5py\\_hl\\dataset.py:163\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     sid \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[1;32m--> 163\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m \u001b[43mh5d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m    166\u001b[0m     dset_id\u001b[38;5;241m.\u001b[39mwrite(h5s\u001b[38;5;241m.\u001b[39mALL, h5s\u001b[38;5;241m.\u001b[39mALL, data)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5d.pyx:137\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to synchronously create dataset (name already exists)"
     ]
    }
   ],
   "source": [
    "modelling.train_model('NN', batch_size=100)\n",
    "modelling.train_model('MultiModal', batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6243e2-8121-4110-bbba-9bf6bd945da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling.evaluate_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db116e54-430c-4e0f-b508-93245af96e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
