{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f94a6-4a5b-44b0-ad84-7b033f0bca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from sys import getsizeof\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea826e78-3f79-4afa-9306-2683f430c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041738f2-119e-4d6a-87bb-59c95d9fdee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_images_path = 'Train'\n",
    "old_train_path = 'train.csv'\n",
    "old_test_img_path = 'Test'\n",
    "old_test_path = 'test.csv'\n",
    "\n",
    "new_image_path = \"new_train\"\n",
    "new_train_path = \"new_train.csv\"\n",
    "new_test_img_path = 'new_test'\n",
    "new_test_path = \"new_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a2d90-bf57-4105-934a-2f718ecf3765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, old_images_path, old_csv_path, new_image_path, new_csv_path):\n",
    "        self.old_images_path = old_images_path\n",
    "        self.new_image_path = new_image_path\n",
    "        self.new_csv_path = new_csv_path\n",
    "        \n",
    "        self.images = [f'{old_images_path}/{image_path}' for image_path in os.listdir(old_images_path)]\n",
    "        self.image_nr = 0\n",
    "        self.df = pd.read_csv(old_csv_path)\n",
    "\n",
    "    def show_image(self, image):\n",
    "        cv.imshow('Image', image)\n",
    "        cv.waitKey(0) \n",
    "        cv.destroyAllWindows() \n",
    "\n",
    "    def crop_images(self, image):\n",
    "        image_half = int(image.shape[0]/2)\n",
    "        topleft_image = image[0:image_half, 0:image_half]\n",
    "        topright_image = image[0:image_half, image_half-1:-1]\n",
    "        bottomleft_image = image[image_half-1:-1, 0:image_half]\n",
    "        bottomright_image = image[image_half-1:-1, image_half-1:-1]\n",
    "        return (topleft_image, topright_image, bottomleft_image, bottomright_image)\n",
    "        \n",
    "    def create_permutations(self, image):\n",
    "        topleft, topright, bottomleft, bottomright = self.crop_images(image)\n",
    "        perms = list(itertools.permutations([topleft, topright, bottomleft, bottomright]))\n",
    "        for permutation in perms:\n",
    "            top_image = np.concatenate((permutation[0], permutation[1]), axis=1)\n",
    "            bottom_image = np.concatenate((permutation[2], permutation[3]), axis=1)\n",
    "            image = np.concatenate((top_image, bottom_image), axis=0)\n",
    "            image = cv.resize(image, (512, 512))\n",
    "            cv.imwrite(f'{self.new_image_path}/{self.image_nr}.jpg', image)\n",
    "            self.image_nr += 1\n",
    "            \n",
    "    def get_new_images(self):\n",
    "        os.mkdir(self.new_image_path)\n",
    "        for image_path in self.images:\n",
    "            image = cv.imread(image_path)\n",
    "            self.create_permutations(image)\n",
    "\n",
    "        for image_path in self.images:\n",
    "            image = cv.imread(image_path)\n",
    "            image = cv.flip(image, 1)\n",
    "            self.create_permutations(image)\n",
    "\n",
    "    def get_new_train_csv(self):\n",
    "        df_duplicates = self.df.reindex(self.df.index.repeat(24)).reset_index(drop=True)\n",
    "\n",
    "        df_duplicates = pd.concat([df_duplicates, df_duplicates])\n",
    "        df_duplicates['House ID'] = range(0, len(df_duplicates))\n",
    "        \n",
    "        df_duplicates = df_duplicates.reset_index(drop=True)\n",
    "        df_duplicates.to_csv(self.new_csv_path, index=False)\n",
    "\n",
    "    def shuffle_images(self):\n",
    "        numbers = list(range(len(os.listdir(self.new_image_path))))\n",
    "        random.shuffle(numbers)\n",
    "        \n",
    "        df = pd.read_csv(self.new_csv_path)\n",
    "        df['House ID'] = numbers\n",
    "        df = df.sort_values('House ID')\n",
    "        df.to_csv(self.new_csv_path, index=False)\n",
    "        \n",
    "        for image_name, new_image_name in zip(list(range(len(os.listdir(self.new_image_path)))), numbers):\n",
    "            os.rename(f'{self.new_image_path}/{image_name}.jpg', f'{self.new_image_path}/a{new_image_name}.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd5edb1-513e-42ed-a16d-f7088cf936a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp = Preprocessing(old_images_path, old_train_path, new_image_path, new_train_path)\n",
    "# pp.get_new_images()\n",
    "# pp.get_new_train_csv()\n",
    "# pp.shuffle_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f7740-50fc-43e7-9d89-8890d39ca504",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelling:\n",
    "    def __init__(self, train_path, image_path, test_path):\n",
    "        self.train_data = pd.read_csv(train_path)\n",
    "        self.test_data = pd.read_csv(test_path) \n",
    "        self.image_path = image_path\n",
    "        self.image_amount = len(os.listdir(image_path))\n",
    "        \n",
    "        self.X_train_csv, self.X_val_csv, self.X_train_img, self.X_val_img, self.y_train, self.y_val = None, None, None, None, None, None\n",
    "        self.X_test = None\n",
    "\n",
    "        self.checkpoint_path = None\n",
    "        self.checkpoint = None\n",
    "\n",
    "        self.models = {'NN': \n",
    "                           {'function': self.create_NN, \n",
    "                            'model': None,\n",
    "                            'data': None,\n",
    "                            'history': pd.DataFrame({'loss': [math.inf], 'val_loss': [math.inf]})},  \n",
    "                       'CNN': \n",
    "                           {'function': self.create_CNN, \n",
    "                            'model': None,\n",
    "                            'data': None,\n",
    "                            'history': pd.DataFrame({'loss': [math.inf], 'val_loss': [math.inf]})}, \n",
    "                       'Transfer': \n",
    "                           {'function': self.create_Transfer, \n",
    "                            'model': None,\n",
    "                            'data': None,\n",
    "                            'history': pd.DataFrame({'loss': [math.inf], 'val_loss': [math.inf]})}, \n",
    "                       'MultiModal': \n",
    "                           {'function': self.create_MultiModal, \n",
    "                            'model': None,\n",
    "                            'data': None,\n",
    "                            'history': pd.DataFrame({'loss': [math.inf], 'val_loss': [math.inf]})}}\n",
    "\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    def prepare_data(self, start, end):\n",
    "        amount_of_images = end - start\n",
    "        scaler = StandardScaler() \n",
    "        \n",
    "        X = self.train_data.drop(['Price'], axis=1)\n",
    "        y = self.train_data['Price']\n",
    "        \n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        images = np.empty((amount_of_images, 512, 512, 3), dtype=np.uint8)\n",
    "        for i, image in enumerate(os.listdir(self.image_path)[start:end]):\n",
    "            image = cv.imread(f'{self.image_path}/{image}')\n",
    "            images[i, ...] = image\n",
    "\n",
    "        self.X_train_csv, self.X_val_csv, self.X_train_img, self.X_val_img, self.y_train, self.y_val = \\\n",
    "        train_test_split(X_scaled[start:end], images, y[start:end], test_size=0.2, random_state=42)\n",
    "\n",
    "        self.models['NN']['data'] = (self.X_train_csv, self.X_val_csv)\n",
    "        self.models['CNN']['data'] = (self.X_train_img, self.X_val_img)\n",
    "        self.models['Transfer']['data'] = ([self.X_train_img,  self.X_train_csv], [self.X_val_img, self.X_val_csv])\n",
    "        self.models['MultiModal']['data'] = ([self.X_train_img,  self.X_train_csv], [self.X_val_img, self.X_val_csv])\n",
    "        \n",
    "        #Standariseren \n",
    "        self.X_test = scaler.transform(self.test_data)\n",
    "\n",
    "    def create_NN(self):\n",
    "        self.models['NN']['model'] = keras.Sequential()\n",
    "\n",
    "        self.models['NN']['model'].add(layers.Input(shape=(6)))\n",
    "        self.models['NN']['model'].add(layers.Dense(256, activation='relu', name='NN_dense_1'))\n",
    "        self.models['NN']['model'].add(layers.Dropout(0.5, name='NN_dropout_1'))\n",
    "        self.models['NN']['model'].add(layers.Dense(128, activation='relu', name='NN_dense_2'))\n",
    "        self.models['NN']['model'].add(layers.Dropout(0.3, name='NN_dropout_2'))\n",
    "        self.models['NN']['model'].add(layers.Dense(64, activation='relu', name='NN_dense_3'))\n",
    "        self.models['NN']['model'].add(layers.Dense(1, name='NN_dense_output'))\n",
    "        \n",
    "        self.models['NN']['model'].compile(optimizer=self.optimizer, loss='mean_absolute_percentage_error')\n",
    "\n",
    "    def create_CNN(self):\n",
    "        self.models['CNN']['model'] = keras.Sequential()\n",
    "        \n",
    "        self.models['CNN']['model'].add(layers.Input(shape=(512, 512, 3), name='CNN_input_img'))\n",
    "        self.models['CNN']['model'].add(layers.experimental.preprocessing.Rescaling(1./255, name='CNN_rescaling'))\n",
    "        self.models['CNN']['model'].add(layers.Conv2D(16, (3, 3), padding='same', activation='relu', name='CNN_conv2d_1'))\n",
    "        self.models['CNN']['model'].add(layers.MaxPooling2D(name='CNN_max_pooling2d_1'))\n",
    "        self.models['CNN']['model'].add(layers.Conv2D(32, 3, padding='same', activation='relu', name='CNN_conv2d_2'))\n",
    "        self.models['CNN']['model'].add(layers.MaxPooling2D(name='CNN_max_pooling2d_2'))\n",
    "        self.models['CNN']['model'].add(layers.Conv2D(64, 3, padding='same', activation='relu', name='CNN_conv2d_3'))\n",
    "        self.models['CNN']['model'].add(layers.MaxPooling2D(name='CNN_max_pooling2d_3'))\n",
    "        self.models['CNN']['model'].add(layers.Flatten(name='CNN_flatten_img'))\n",
    "        self.models['CNN']['model'].add(layers.Dense(128, activation='relu', name='CNN_dense_combined'))\n",
    "        self.models['CNN']['model'].add(layers.Dense(1, name='CNN_dense_output'))\n",
    "\n",
    "        self.models['CNN']['model'].compile(optimizer=self.optimizer, loss='mean_absolute_percentage_error')\n",
    "        \n",
    "    def create_Transfer(self):\n",
    "        pass\n",
    "        \n",
    "    def create_MultiModal(self):\n",
    "        img_input = layers.Input(shape=(512, 512, 3), name='MM_input_img')\n",
    "        csv_input = layers.Input(shape=(6), name='MM_input_csv')\n",
    "        \n",
    "        x_img = layers.experimental.preprocessing.Rescaling(1./255, name='MM_rescaling')(img_input)\n",
    "        x_img = layers.Conv2D(16, (3, 3), padding='same', activation='relu', name='MM_conv2d_1')(x_img)\n",
    "        x_img = layers.MaxPooling2D(name='MM_max_pooling2d_1')(x_img)\n",
    "        x_img = layers.Conv2D(32, 3, padding='same', activation='relu', name='MM_conv2d_2')(x_img)\n",
    "        x_img = layers.MaxPooling2D(name='MM_max_pooling2d_2')(x_img)\n",
    "        x_img = layers.Conv2D(64, 3, padding='same', activation='relu', name='MM_conv2d_3')(x_img)\n",
    "        x_img = layers.MaxPooling2D(name='MM_max_pooling2d_3')(x_img)\n",
    "        x_img = layers.Flatten(name='MM_flatten_img')(x_img)\n",
    "        \n",
    "        x_csv = layers.Flatten(name='MM_flatten_csv')(csv_input)\n",
    "        x_csv = layers.Dense(16, activation='relu', name='MM_dense_1')(x_csv)\n",
    "        x_csv = layers.Dense(32, activation='relu', name='MM_dense_2')(x_csv)\n",
    "        x_csv = layers.Dense(64, activation='relu', name='MM_dense_3')(x_csv)\n",
    "        \n",
    "        x = layers.concatenate([x_img, x_csv], name='MM_concatenate')\n",
    "        x = layers.Dense(128, activation='relu', name='MM_dense_combined')(x)\n",
    "        output = layers.Dense(1, name='MM_dense_output')(x)\n",
    "        \n",
    "        # make model with 2 inputs and 1 output\n",
    "        self.models['MultiModal']['model'] = tf.keras.models.Model(inputs=[img_input, csv_input], outputs=output)\n",
    "\n",
    "        self.models['MultiModal']['model'].compile(optimizer=self.optimizer, loss='mean_absolute_percentage_error')\n",
    "        \n",
    "    def train_models(self, models, batch_size=1000):  \n",
    "        for model in models:\n",
    "            try:\n",
    "                self.models[model]['function']()\n",
    "            except:\n",
    "                print('This model does not exist!')\n",
    "            \n",
    "            for i in range(0, self.image_amount, batch_size):\n",
    "                start = i\n",
    "                end = min(i + batch_size, self.image_amount)\n",
    "                print(f'Training {model} on images {start} to {end}...')\n",
    "                tf.keras.backend.clear_session()\n",
    "                \n",
    "                self.prepare_data(start, end)\n",
    "                \n",
    "                self.checkpoint_path = f\"{model}_checkpoint.h5\"\n",
    "                self.checkpoint = ModelCheckpoint(self.checkpoint_path, monitor='val_loss', verbose=1, \\\n",
    "                                                  save_best_only=True, mode='min', \\\n",
    "                                                  initial_value_threshold=self.models[model]['history']['val_loss'].iloc[-1])\n",
    "                \n",
    "                self.models[model]['model'].fit(self.models[model]['data'][0], self.y_train, epochs=50, batch_size=16, \\\n",
    "                                                validation_data=(self.models[model]['data'][1], self.y_val), \\\n",
    "                                                callbacks=[self.early_stopping, self.checkpoint], verbose=0)\n",
    "                gc.collect()\n",
    "    \n",
    "                history = pd.DataFrame(self.models[model]['model'].history.history)\n",
    "                self.models[model]['history'] = pd.concat([self.models[model]['history'], history], ignore_index=True)\n",
    "                self.models[model]['history']['loss'] = self.history_processing(self.models[model]['history']['loss'])\n",
    "                self.models[model]['history']['val_loss'] = self.history_processing(self.models[model]['history']['val_loss'])\n",
    "\n",
    "    def history_processing(self, history):\n",
    "        new_history = [history[0]]\n",
    "        for num in history[1:]: \n",
    "            new_history.append(min(num, new_history[-1])) \n",
    "        return new_history\n",
    "\n",
    "    \n",
    "    def evaluate_models(self):\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        for model in self.models.keys():\n",
    "            plt.tight_layout()\n",
    "            ax[0].plot(self.models[model]['history']['loss'][1:], label=f'{model} Train Loss')\n",
    "            ax[0].set_xlabel('Epochs')\n",
    "            ax[0].set_ylabel('Mean Absolute Percentage Error (mape)')\n",
    "            ax[0].legend()\n",
    "            \n",
    "            ax[1].plot(self.models[model]['history']['val_loss'][1:], label=f'{model} Val Loss')\n",
    "            ax[1].set_xlabel('Epochs')\n",
    "            ax[1].set_ylabel('Mean Absolute Percentage Error (mape)')\n",
    "            ax[1].legend()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7720ddd-4f38-4ddf-88ad-aa1e76f55d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling = Modelling(new_train_path, new_image_path, old_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3db602d-df35-400e-abf9-208ce1406271",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling.train_models(['CNN'], batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d1f2cc-b81d-4bd6-abc3-d308ed8e7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling.train_models(['NN', 'CNN', 'MultiModal'], batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6243e2-8121-4110-bbba-9bf6bd945da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling.evaluate_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a231c6f5-a9f3-49e6-ad8f-301a5e0d66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350280bb-b237-43e4-a24f-14d7fac83d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d8ced-dc29-4323-b255-cb65a1d75519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
