{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLops\n",
    "Door Mark Olieman en Tommi Lander\n",
    "\n",
    "Dit notebook bevat alles wat ook in de .py bestanden staat. Echter staat in de requirements voor het nakijken op brightspace dat het in dit formaat moet staan anders wordt het niet nagekeken. Dus bij deze dit formaat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (4.10.0.82)\n",
      "Requirement already satisfied: cvzone in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: ultralytics in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (8.2.32)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (6.0.1)\n",
      "Collecting moviepy\n",
      "  Using cached moviepy-1.0.3-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from ultralytics) (3.9.0)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from ultralytics) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from ultralytics) (2.3.1+cu118)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from ultralytics) (0.18.1+cu118)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from ultralytics) (4.66.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=0.2.5 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from ultralytics) (0.2.8)\n",
      "Collecting decorator<5.0,>=4.0.2 (from moviepy)\n",
      "  Using cached decorator-4.4.2-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting proglog<=1.0.0 (from moviepy)\n",
      "  Using cached proglog-0.1.10-py3-none-any.whl.metadata (639 bytes)\n",
      "Collecting imageio<3.0,>=2.5 (from moviepy)\n",
      "  Using cached imageio-2.34.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting imageio-ffmpeg>=0.2.0 (from moviepy)\n",
      "  Using cached imageio_ffmpeg-0.5.1-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (69.5.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Using cached imageio-2.34.1-py3-none-any.whl (313 kB)\n",
      "Using cached imageio_ffmpeg-0.5.1-py3-none-win_amd64.whl (22.6 MB)\n",
      "Using cached proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Installing collected packages: imageio-ffmpeg, imageio, decorator, proglog, moviepy\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "Successfully installed decorator-4.4.2 imageio-2.34.1 imageio-ffmpeg-0.5.1 moviepy-1.0.3 proglog-0.1.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (2.3.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (0.18.1+cu118)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (2.3.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torchvision) (1.26.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\mark\\miniconda3\\envs\\test\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas scikit-learn opencv-python cvzone ultralytics pyyaml moviepy\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \n",
    "# deze import is om local te kunnen runnen op je gpu\n",
    "# als je de local gpu import meeneemt kan deze cel 20+ minuten duren om te runnen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: inhoudsopgave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import shutil\n",
    "import yaml # type: ignore\n",
    "import os\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "import numpy as np # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "\n",
    "import moviepy.editor as mp # type: ignore\n",
    "from ultralytics import YOLO # type: ignore\n",
    "import cv2 # type: ignore\n",
    "import cvzone # type: ignore\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Labeler: labeler.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uitleg van de code\n",
    "\n",
    "Deze code bevat twee hoofdonderdelen: een functie om een video te resizen en een klasse `imagePainter` die frames uit een video haalt en de mogelijkheid biedt om rechthoeken op deze frames te tekenen en op te slaan.\n",
    "\n",
    "#### Functie: `resize_video`\n",
    "\n",
    "De `resize_video` functie neemt een pad naar een video als invoer, resize de video naar een resolutie van 1280x720, en slaat de resized video op in de map `new_data/resized_videos`. Hier zijn de belangrijkste stappen:\n",
    "\n",
    "1. Laad de video met `VideoFileClip` uit de `moviepy` bibliotheek.\n",
    "2. Bepaal het nieuwe pad voor de resized video.\n",
    "3. Resize de video naar 1280x720 pixels.\n",
    "4. Sla de resized video op met de codec 'libx264'.\n",
    "5. Geef het pad naar de nieuwe video terug.\n",
    "\n",
    "#### Class: `imagePainter`\n",
    "\n",
    "De `imagePainter` klasse heeft als doel om frames uit een video te halen en rechthoeken op deze frames te tekenen. Hieronder volgt een uitleg van de belangrijkste methoden en attributen:\n",
    "\n",
    "- **Constructor (`__init__`)**:\n",
    "  - Initialiseer de klasse met het pad naar de video, het startframe, en een optionele titel.\n",
    "  - Stel paden en namen in voor de video en de coördinaten van de rechthoeken.\n",
    "  - Controleer of er al een CSV-bestand met coördinaten bestaat, zo niet, maak een nieuw DataFrame aan.\n",
    "  - Roep de methode `draw_on_frames_from_video` aan om frames te verwerken.\n",
    "\n",
    "- **Methode: `create_folders`**:\n",
    "  - Maak mappen aan voor de data, frames en face-frames als deze nog niet bestaan.\n",
    "  - Gebruik `check_and_create_folder` om te controleren of de map al bestaat en maak deze aan indien nodig.\n",
    "\n",
    "- **Methode: `check_and_create_folder`**:\n",
    "  - Probeer een map aan te maken en geef een melding als de map al bestaat.\n",
    "\n",
    "- **Methode: `get_data_paths`**:\n",
    "  - Bepaal de paden voor het opslaan van frames en face-frames op basis van het framenummer.\n",
    "\n",
    "- **Methode: `draw_on_frames_from_video`**:\n",
    "  - Verwerk de video frame voor frame vanaf het opgegeven startframe.\n",
    "  - Laat de gebruiker rechthoeken tekenen op de frames.\n",
    "  - Sla de frames en rechthoekcoördinaten op in CSV-bestanden.\n",
    "\n",
    "- **Methode: `image_painter`**:\n",
    "  - Voor elk frame, laat de gebruiker rechthoeken tekenen en sla de resultaten op.\n",
    "  - Stel een muiscallback in om rechthoeken te kunnen tekenen (`draw_rectangle_with_drag`).\n",
    "\n",
    "- **Methode: `draw_rectangle_with_drag`**:\n",
    "  - Handelt muisgebeurtenissen af om rechthoeken te tekenen op de frames.\n",
    "  - Slaat de coördinaten van de getekende rechthoek op in een DataFrame.\n",
    "\n",
    "De cel hieronder doet alles om het train proces te kunnen uitvoeren, alles behalve het kiezen van de video. Dat wordt in de app uitgevoerd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_video(path):\n",
    "    clip = mp.VideoFileClip(path)\n",
    "    path = os.path.basename(path)\n",
    "    new_path = f'new_data/resized_videos/{path}'\n",
    "    clip_resized = clip.resize((1280, 720))\n",
    "    clip_resized.write_videofile(new_path, codec='libx264')\n",
    "    return new_path\n",
    "    \n",
    "class imagePainter:\n",
    "    def __init__(self, video_path, frame_start, title='ImagePainter!'):\n",
    "        self.running = True\n",
    "        \n",
    "        self.title = title\n",
    "        self.frame_start = frame_start\n",
    "        \n",
    "        self.video_name = os.path.basename(video_path).rstrip('.mp4')\n",
    "        self.video_path = video_path.rstrip(self.video_name)\n",
    "        self.coords_path = f'new_data/coords.csv'\n",
    "        \n",
    "        if 'coords.csv' not in os.listdir('new_data/'):\n",
    "            self.rectangle_coords = pd.DataFrame({'image': [], 'x1': [], 'y1': [], 'x2': [], 'y2': []})\n",
    "            self.rectangle_coords.to_csv(self.coords_path)\n",
    "        else:\n",
    "            self.rectangle_coords = pd.read_csv(self.coords_path)\n",
    "        \n",
    "        self.draw_on_frames_from_video(video_path)\n",
    "        \n",
    "    def create_folders(self):\n",
    "        data_folder = 'data'\n",
    "        frame_folder = f'{data_folder}/{self.video_name}_frames'\n",
    "        face_folder =  f'{frame_folder}_face_boxes'\n",
    "\n",
    "        self.check_and_create_folder(data_folder)\n",
    "        self.check_and_create_folder(frame_folder)\n",
    "        self.check_and_create_folder(face_folder)\n",
    "        \n",
    "        return (frame_folder, face_folder, data_folder)\n",
    "    \n",
    "    def check_and_create_folder(self, folder):\n",
    "        try:\n",
    "            os.mkdir(folder)\n",
    "            print(f'Creating {folder}')\n",
    "        except FileExistsError:\n",
    "            print(f'{folder} folder already exists! skipping creation...')\n",
    "            \n",
    "    def get_data_paths(self, count):\n",
    "        frame_path = f'new_data/frames/{self.video_name}_frame_{count}.jpg'\n",
    "        face_path = f'new_data/face_frames/{self.video_name}_face_frame_{count}.jpg'\n",
    "        \n",
    "        return (frame_path, face_path)\n",
    "             \n",
    "    def draw_on_frames_from_video(self, video_path):\n",
    "        self.show_previous_frame = False\n",
    "        vid = cv2.VideoCapture(video_path)\n",
    "        vid.set(cv2.CAP_PROP_POS_FRAMES, self.frame_start)\n",
    "        frame_count = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        count = self.frame_start\n",
    "        \n",
    "        while self.running and vid.isOpened():\n",
    "            self.title = f'ImagePainter - {self.video_name} - Image{count}/{frame_count}'  \n",
    "            if self.show_previous_frame:\n",
    "                count -= 1\n",
    "                vid.set(cv2.CAP_PROP_POS_FRAMES, current_frame-1)\n",
    "                self.running, self.img = vid.read()\n",
    "                self.show_previous_frame = False\n",
    "            else:\n",
    "                self.running, self.img = vid.read()\n",
    "                \n",
    "                \n",
    "            next_frame = vid.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "            current_frame = next_frame - 1\n",
    "            \n",
    "            try:\n",
    "                self.original_img = self.img.copy()\n",
    "            except AttributeError:\n",
    "                cv2.destroyAllWindows() \n",
    "                self.rectangle_coords.to_csv(self.coords_path, index=False)\n",
    "                break\n",
    "            self.drawing_img = self.img.copy()\n",
    "\n",
    "            self.image_painter(count)\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "        self.rectangle_coords.to_csv(self.coords_path, index=False)\n",
    "        \n",
    "    def image_painter(self, count):\n",
    "        self.image_name = f'{self.video_name}_frame_{count}.jpg'\n",
    "        frame_path, face_path = self.get_data_paths(count)\n",
    "        \n",
    "        self.ix = -1\n",
    "        self.iy = -1\n",
    "        self.drawing = False\n",
    "        \n",
    "        while self.running:\n",
    "            cv2.namedWindow(winname = self.title) \n",
    "            cv2.setMouseCallback(self.title,  self.draw_rectangle_with_drag) \n",
    "            cv2.imshow(self.title, self.img)\n",
    "                \n",
    "            key = cv2.waitKey(0)\n",
    "            if key == 27:\n",
    "                self.running = False\n",
    "                cv2.destroyAllWindows() \n",
    "                self.rectangle_coords.to_csv(self.coords_path, index=False)\n",
    "            else:\n",
    "                cv2.imwrite(frame_path, self.original_img)\n",
    "                cv2.imwrite(face_path, self.drawing_img)\n",
    "                cv2.destroyAllWindows()  \n",
    "                break\n",
    "    \n",
    "    def draw_rectangle_with_drag(self, event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN: \n",
    "            self.drawing = True\n",
    "            self.ix = x \n",
    "            self.iy = y\n",
    "            \n",
    "        elif event == cv2.EVENT_MOUSEMOVE: \n",
    "            if self.drawing == True: \n",
    "                self.drawing_img = self.img.copy()\n",
    "                cv2.rectangle(self.drawing_img, \n",
    "                              pt1=(self.ix, self.iy), pt2=(x, y), \n",
    "                              color=(0, 0, 255), \n",
    "                              thickness=1, \n",
    "                              lineType=cv2.LINE_AA)\n",
    "                cv2.imshow(self.title, self.drawing_img)\n",
    "        \n",
    "        elif event == cv2.EVENT_LBUTTONUP: \n",
    "            self.img = self.drawing_img\n",
    "            coords = pd.DataFrame({'image': [self.image_name], 'x1': [self.ix], 'y1': [self.iy], 'x2': [x], 'y2': [y]})\n",
    "            self.rectangle_coords = pd.concat([self.rectangle_coords, coords], axis=0)\n",
    "            self.drawing = False\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data verwerking: data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uitleg van de code\n",
    "\n",
    "Deze code bevat verschillende functies en methoden om een dataset voor te bereiden voor een YOLOv1 modeltraining. Hieronder volgt een uitleg van de verschillende onderdelen van de code.\n",
    "\n",
    "### Functie: `csv_to_labels`\n",
    "\n",
    "Deze functie verwerkt een CSV-bestand met annotaties van bounding boxes om de data geschikt te maken voor invoer in een YOLOv8n model.\n",
    "\n",
    "#### Stappen:\n",
    "\n",
    "1. **Lees de CSV**:\n",
    "   - Laad de CSV met de annotaties in een DataFrame.\n",
    "   \n",
    "2. **Bereken de middelpuntcoördinaten en afmetingen**:\n",
    "   - Bereken de middenpunten (`x`, `y`) en de breedte en hoogte (`w`, `h`) van de bounding boxes.\n",
    "   \n",
    "3. **Pas coördinaten aan als ze buiten het beeld vallen**:\n",
    "   - Corrigeer waarden die buiten de beeldresolutie van 1280x720 vallen.\n",
    "   \n",
    "4. **Normaliseer de coördinaten**:\n",
    "   - Deel de coördinaten door de breedte en hoogte van het beeld om ze te normaliseren.\n",
    "   \n",
    "5. **Schrijf de aangepaste annotaties naar bestanden**:\n",
    "   - Voor elke rij in de DataFrame, schrijf de genormaliseerde coördinaten naar een tekstbestand in de map `new_data/labels`.\n",
    "\n",
    "### Functie: `create_training_run`\n",
    "\n",
    "Deze functie bereidt een nieuwe trainingssessie voor door de benodigde mappen en bestanden aan te maken en de gegevens op te splitsen in trainings- en validatiesets.\n",
    "\n",
    "#### Stappen:\n",
    "\n",
    "1. **Verwijder bestaande face-frames en resized video’s**:\n",
    "   - Verwijder alle bestanden in de mappen `new_data/face_frames` en `new_data/resized_videos`.\n",
    "   \n",
    "2. **Verwijder het bestaande coördinaten CSV-bestand**:\n",
    "   - Probeer `new_data/coords.csv` te verwijderen.\n",
    "   \n",
    "3. **Maak een nieuwe trainingsmap aan**:\n",
    "   - Bepaal het nummer voor de nieuwe trainingsrun en maak de bijbehorende mappen aan.\n",
    "   \n",
    "4. **Splits de data in training en validatie**:\n",
    "   - Verdeel de beelden willekeurig in een trainings- en validatieset (80% training, 20% validatie).\n",
    "   \n",
    "5. **Verplaats de bestanden naar de juiste mappen**:\n",
    "   - Verplaats de afbeeldingen en hun labels naar de juiste trainings- en validatiemappen.\n",
    "   \n",
    "6. **Maak een YAML-bestand aan**:\n",
    "   - Roep `create_yaml` aan om een YAML-configuratiebestand te maken voor de trainingsrun.\n",
    "\n",
    "### Functie: `get_highest_training_run`\n",
    "\n",
    "Deze functie bepaalt het hoogste nummer van de trainingsrun die al bestaat.\n",
    "\n",
    "#### Stappen:\n",
    "\n",
    "1. **Lees de bestaande trainingsruns**:\n",
    "   - Lees alle mappen in `Training_runs` en haal de hoogste numerieke suffix op.\n",
    "   \n",
    "2. **Geef het hoogste trainingsrun-nummer terug**:\n",
    "   - Geef de naam van de trainingsrun met het hoogste nummer terug.\n",
    "\n",
    "### Functie: `create_yaml`\n",
    "\n",
    "Deze functie maakt een YAML-configuratiebestand aan voor de meest recente trainingsrun.\n",
    "\n",
    "#### Stappen:\n",
    "\n",
    "1. **Bepaal de hoogste trainingsrun**:\n",
    "   - Roep `get_highest_training_run` aan om de naam van de meest recente trainingsrun te krijgen.\n",
    "   \n",
    "2. **Maak de YAML-inhoud aan**:\n",
    "   - Maak de inhoud van het YAML-bestand aan met de paden naar de trainings- en validatiemappen en de naam van de klasse (`face`).\n",
    "   \n",
    "3. **Schrijf het YAML-bestand**:\n",
    "   - Schrijf de YAML-inhoud naar een bestand in de map van de hoogste trainingsrun.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_labels():\n",
    "  \"\"\"\n",
    "  Preprocesses a CSV file containing bounding box annotations to prepare\n",
    "  the data for YOLOv1 model input with a 16x9 grid size.\n",
    "\n",
    "  Args:\n",
    "      csv_path (str): Path to the CSV file containing bounding box annotations.\n",
    "      image_size (tuple): Size of the original images (height, width).\n",
    "\n",
    "  Returns:\n",
    "      tuple: Tuple of numpy arrays (X, Y). X represents the grids and Y represents the bounding boxes.\n",
    "  \"\"\"\n",
    "  image_w, image_h = (1280, 720)\n",
    "  df = pd.read_csv('new_data/coords.csv')\n",
    "  \n",
    "  df['x'] = (df['x1']+df['x2']) /2\n",
    "  df['y'] = (df['y1']+df['y2']) /2\n",
    "  df['w'] = df['x2'] - df['x1']\n",
    "  df['h'] = df['y2'] - df['y1']\n",
    "  \n",
    "  df = df.drop(columns=['x1', 'y1', 'x2', 'y2'])\n",
    "  \n",
    "  df['y'] = df['y'].apply(lambda x: 1279 if x >= 1280 else x)\n",
    "  df['x'] = df['x'].apply(lambda x: 719 if x >= 720 else x)\n",
    "\n",
    "  # Normalize bounding box coordinates\n",
    "  df['x'] = df['x'] / image_w\n",
    "  df['y'] = df['y'] / image_h\n",
    "  df['w'] = df['w'] / image_w\n",
    "  df['h'] = df['h'] / image_h\n",
    "    \n",
    "  for _, row in df.iterrows():\n",
    "    path = 'new_data/labels/' + row['image']\n",
    "    path = path.rstrip('.jpg')\n",
    "    with open(path + '.txt', 'a') as file:\n",
    "      x = row['x']\n",
    "      y = row['y']\n",
    "      w = row['w']\n",
    "      h = row['h']\n",
    "      file.write(f'0 {x} {y} {w} {h}')\n",
    "      file.write('\\n')\n",
    "\n",
    "def create_training_run():\n",
    "  face_frames_folder = 'new_data/face_frames/'\n",
    "  resized_videos_folder = 'new_data/resized_videos/'\n",
    "  \n",
    "  for face_frame in os.listdir(face_frames_folder):\n",
    "    file_path = os.path.join(face_frames_folder, face_frame)\n",
    "    os.unlink(file_path)\n",
    "    \n",
    "  for video in os.listdir(resized_videos_folder):\n",
    "    file_path = os.path.join(resized_videos_folder, video)\n",
    "    os.unlink(file_path)\n",
    "  \n",
    "  try:\n",
    "    os.unlink('new_data/coords.csv')\n",
    "  except:\n",
    "    pass\n",
    "    \n",
    "  training_folder = 'Training_runs'\n",
    "  \n",
    "  image_folder = 'new_data/frames'\n",
    "  label_folder = 'new_data/labels'\n",
    "  \n",
    "  counter = 1\n",
    "  new_training_path = f\"{training_folder}/training_run_{counter}\"\n",
    "  \n",
    "  \n",
    "  \n",
    "  while os.path.exists(new_training_path):\n",
    "      counter += 1\n",
    "      new_training_path = f\"{training_folder}/training_run_{counter}\"\n",
    "  os.makedirs(new_training_path)\n",
    "  \n",
    "  train_image_folder = f'{new_training_path}/train/images'\n",
    "  val_image_folder = f'{new_training_path}/val/images'\n",
    "  train_label_folder = f'{new_training_path}/train/labels'\n",
    "  val_label_folder = f'{new_training_path}/val/labels'\n",
    "  \n",
    "  os.makedirs(train_image_folder)\n",
    "  os.makedirs(val_image_folder)\n",
    "  os.makedirs(train_label_folder)\n",
    "  os.makedirs(val_label_folder)\n",
    "  \n",
    "  images = [f for f in os.listdir(image_folder)]\n",
    "  \n",
    "  # Shuffle images for random splitting\n",
    "  random.shuffle(images)\n",
    "  \n",
    "  # Determine the split index\n",
    "  split_index = int(len(images) * (1 - 0.2))\n",
    "\n",
    "  # Split into training and validation sets\n",
    "  train_images = images[:split_index]\n",
    "  val_images = images[split_index:]\n",
    "\n",
    "  # Move files to their respective directories\n",
    "  for img in train_images:\n",
    "      shutil.move(os.path.join(image_folder, img), os.path.join(train_image_folder, img))\n",
    "      shutil.move(os.path.join(label_folder, os.path.splitext(img)[0] + '.txt'), os.path.join(train_label_folder, os.path.splitext(img)[0] + '.txt'))\n",
    "  \n",
    "  for img in val_images:\n",
    "      shutil.move(os.path.join(image_folder, img), os.path.join(val_image_folder, img))\n",
    "      shutil.move(os.path.join(label_folder, os.path.splitext(img)[0] + '.txt'), os.path.join(val_label_folder, os.path.splitext(img)[0] + '.txt'))\n",
    "    \n",
    "  create_yaml()\n",
    "  \n",
    "def get_highest_training_run():\n",
    "    training_run_folders = [f for f in os.listdir('Training_runs/')]\n",
    "    training_run_numbers = [int(f.split('_')[-1]) for f in training_run_folders if f.split('_')[-1].isnumeric()]\n",
    "    try:\n",
    "        highest_number = max(training_run_numbers)\n",
    "    except:\n",
    "        highest_number = 0\n",
    "    return f\"training_run_{highest_number}\"\n",
    "\n",
    "def create_yaml():\n",
    "  highest_training_run = get_highest_training_run()\n",
    "  yaml_content = {\n",
    "      'path': 'training_runs',\n",
    "      'train': f\"{highest_training_run}/train\",\n",
    "      'val': f\"{highest_training_run}/val\",\n",
    "      'names': {\n",
    "          0: 'face'\n",
    "      }\n",
    "  }\n",
    "  with open(os.path.join(f'Training_runs/{highest_training_run}', f'{highest_training_run}.yaml'), 'w') as yaml_file:\n",
    "      yaml.dump(yaml_content, yaml_file, default_flow_style=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training: train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uitleg van de code\n",
    "\n",
    "De functie `train_model` traint een YOLO-model (You Only Look Once) op basis van de meest recente trainingsrun. Hieronder volgt een gedetailleerde uitleg van de verschillende stappen in de functie.\n",
    "\n",
    "### Functie: `train_model`\n",
    "\n",
    "#### Stappen:\n",
    "\n",
    "1. **Schakel Weights & Biases uit**:\n",
    "   - Zet de `WANDB_DISABLED` omgevingsvariabele op 'true' om te voorkomen dat Weights & Biases wordt gebruikt tijdens de training.\n",
    "   \n",
    "2. **Bepaal de meest recente trainingsrun**:\n",
    "   - Roep de functie `get_highest_training_run` aan om de naam van de meest recente trainingsrun te krijgen.\n",
    "   \n",
    "3. **Bepaal het pad naar het YAML-configuratiebestand**:\n",
    "   - Stel het pad in naar het YAML-bestand dat is aangemaakt voor de meest recente trainingsrun.\n",
    "   \n",
    "4. **Laad het YOLO-model**:\n",
    "   - Laad een vooraf getraind YOLO-model met de beste gewichten (`best.pt`) uit een de laatste training run.\n",
    "   \n",
    "5. **Train het model**:\n",
    "   - Probeer het model te trainen met de configuratie uit het YAML-bestand, voor 50 epochs, met enkele klasse (`single_cls=True`), in rechthoekige modus (`rect=True`), zonder plots (`plots=False`), en met een afbeeldingsgrootte van 720x1280 pixels.\n",
    "   - Specificeer het project (`project='Training_runs'`) en de naam van de trainingssessie (`name=f'{highest_number_training_run}_training'`).\n",
    "   - Probeer eerst de GPU (`device=0`) te gebruiken. Als dat mislukt, val dan terug op de CPU (`device='cpu'`).\n",
    "\n",
    "### Overzicht\n",
    "\n",
    "Deze functie zorgt ervoor dat het model wordt getraind met behulp van de gegevens en configuratie van de meest recente trainingsrun. Het gebruikt de YOLO-bibliotheek om het model te laden en te trainen, en behandelt eventuele fouten door automatisch over te schakelen van GPU naar CPU als dat nodig is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    os.environ['WANDB_DISABLED'] = 'true'\n",
    "    highest_number_training_run = get_highest_training_run()\n",
    "    \n",
    "    yaml_path = f'Training_runs/{highest_number_training_run}/{highest_number_training_run}.yaml'\n",
    "    # Load and train the YOLO model\n",
    "    model = YOLO('Data/training/weights/best.pt')\n",
    "    \n",
    "    # Train the model using the provided YAML configuration\n",
    "    try:\n",
    "        model.train(data=yaml_path, epochs=50, single_cls=True, rect=True, plots=False, imgsz=[720, 1280], project=f'Training_runs', name=f'{highest_number_training_run}_training', device=0)\n",
    "    except:\n",
    "        model.train(data=yaml_path, epochs=50, single_cls=True, rect=True, plots=False, imgsz=[720, 1280], project=f'Training_runs', name=f'{highest_number_training_run}_training', device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detection: detection.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uitleg van de code\n",
    "\n",
    "De functie `show_detection` gebruikt een webcam om realtime objectdetectie uit te voeren met een getraind YOLO-model. Hieronder volgt de gedetailleerde uitleg van de verschillende stappen in de functie.\n",
    "\n",
    "### Functie: `show_detection`\n",
    "\n",
    "#### Stappen:\n",
    "\n",
    "1. **Start de webcam**:\n",
    "   - Gebruik `cv2.VideoCapture(0)` om de webcam te starten.\n",
    "   - Stel de resolutie van de webcam in op 1280x720 pixels met `cap.set(3, 1280)` en `cap.set(4, 720)`.\n",
    "   \n",
    "2. **Bepaal de meest recente trainingsrun**:\n",
    "   - Roep de functie `get_highest_training_run` aan om de naam van de meest recente trainingsrun te krijgen.\n",
    "\n",
    "3. **Laad het YOLO-model**:\n",
    "   - Als er geen trainingsruns zijn, laad dan de vooraf door ons getrainde model (`'Data/training/weights/best.pt'`)\n",
    "   - Als er wel trainingsruns zijn, laad dan het model uit de meest recente trainingsrun (`{highest_training_run}_training/weights/best.pt`).\n",
    "\n",
    "4. **Definieer klasse namen**:\n",
    "   - Definieer een lijst met class names voor het model om te weten wat er allemaal gedetecteerd moet worden (`classNames`), in dit geval alleen \"face\".\n",
    "\n",
    "5. **Lees frames van de webcam en voer detectie uit**:\n",
    "   - Lees continu frames van de webcam met `cap.read()`.\n",
    "   - Gebruik het YOLO-model om objecten in het frame te detecteren (`model(img, stream=True)`).\n",
    "   - Voor elke detectie, teken een rechthoek om de gedetecteerde objecten met `cvzone.cornerRect`.\n",
    "   - Voeg de klassenamen en vertrouwen (confidence) toe aan het beeld met `cvzone.putTextRect`.\n",
    "\n",
    "6. **Toon het beeld**:\n",
    "   - Toon het beeld met de getekende rechthoeken en tekst in een venster met `cv2.imshow(\"Image\", img)`.\n",
    "\n",
    "7. **Beëindig de detectie**:\n",
    "   - Als de 'q'-toets wordt ingedrukt, stop dan de webcam en sluit alle vensters met `cap.release()` en `cv2.destroyAllWindows()`.\n",
    "\n",
    "### Overzicht\n",
    "\n",
    "Deze functie zorgt ervoor dat de webcam continu frames leest en deze frames worden verwerkt door een YOLO-model om objecten (gezichten) in realtime te detecteren. Gedetecteerde objecten worden gemarkeerd met rechthoeken en vertrouwen wordt weergegeven op het beeld. De detectie stopt wanneer de gebruiker op de 'q'-toets drukt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_detection():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(3, 1280)\n",
    "    cap.set(4, 720)\n",
    "    \n",
    "    highest_training_run = get_highest_training_run()\n",
    "    \n",
    "    if not os.listdir('Training_runs'):\n",
    "        model = YOLO('Data/training/weights/best.pt')\n",
    "    else:\n",
    "        model = YOLO(f'Training_runs/{highest_training_run}_training/weights/best.pt')\n",
    "\n",
    "\n",
    "    classNames = [\"face\"]\n",
    "\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        results = model(img, stream=True)\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                w, h = x2-x1, y2-y1\n",
    "                cvzone.cornerRect(img, (x1, y1, w, h))\n",
    "\n",
    "                conf = math.ceil((box.conf[0]*100))/100\n",
    "\n",
    "                cls = box.cls[0]\n",
    "                name = classNames[int(cls)]\n",
    "\n",
    "                cvzone.putTextRect(\n",
    "                    img, f'{name} 'f'{conf}', (max(0, x1), max(35, y1)), scale=2, thickness=2, \n",
    "                    colorT=(255,255,255), colorR=(54,250,74))\n",
    "\n",
    "        cv2.imshow(\"Image\", img)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The App: app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uitleg van de code\n",
    "\n",
    "Deze code maakt een eenvoudige GUI (Graphical User Interface) voor objectdetectie, met behulp van Tkinter. De GUI bevat drie knoppen die verschillende acties uitvoeren: het labelen van een nieuwe video, het hertrainen van het model en het detecteren van gezichten vanuit een livestream. Hieronder volgt een gedetailleerde uitleg van de verschillende onderdelen van de code.\n",
    "\n",
    "### Functie: `button1_action`\n",
    "\n",
    "Deze functie wordt uitgevoerd wanneer op de knop \"Label new video\" wordt gedrukt.\n",
    "\n",
    "#### Stappen:\n",
    "\n",
    "1. **Vraag de gebruiker om een videobestand te selecteren**:\n",
    "   - Gebruik `filedialog.askopenfilename` om een bestandsdialoog te openen waarin de gebruiker een MP4-bestand kan selecteren.\n",
    "   \n",
    "2. **Pas de grootte van de video aan**:\n",
    "   - Roep `resize_video(file_path)` aan om de geselecteerde video te verkleinen naar een resolutie van 1280x720 pixels.\n",
    "   \n",
    "3. **Start de `imagePainter`**:\n",
    "   - Gebruik `imagePainter` om het labelen van de frames van de nieuwe video te starten vanaf frame 0.\n",
    "   \n",
    "4. **Converteer de gelabelde coördinaten naar YOLO-formaat**:\n",
    "   - Roep `csv_to_labels()` aan om de coördinaten van de gelabelde gezichten om te zetten naar een formaat dat geschikt is voor YOLO.\n",
    "\n",
    "### Functie: `button2_action`\n",
    "\n",
    "Deze functie wordt uitgevoerd wanneer op de knop \"Retrain model\" wordt gedrukt.\n",
    "\n",
    "#### Stappen:\n",
    "\n",
    "1. **Maak een nieuwe trainingsrun aan**:\n",
    "   - Roep `create_training_run()` aan om een nieuwe trainingsrun aan te maken en de benodigde mappen en bestanden in te stellen.\n",
    "   \n",
    "2. **Train het model opnieuw**:\n",
    "   - Roep `train_model()` aan om het YOLO-model opnieuw te trainen met de nieuwste gegevens.\n",
    "\n",
    "### Functie: `button3_action`\n",
    "\n",
    "Deze functie wordt uitgevoerd wanneer op de knop \"Detect faces from livestream\" wordt gedrukt.\n",
    "\n",
    "#### Stappen:\n",
    "\n",
    "1. **Start de detectie van gezichten vanuit een livestream**:\n",
    "   - Roep `show_detection()` aan om de webcam te starten en gezichten in realtime te detecteren en weer te geven.\n",
    "\n",
    "### Hoofdvenster van de GUI\n",
    "\n",
    "1. **Maak het hoofdvenster aan**:\n",
    "   - Gebruik `tk.Tk()` om een nieuw hoofdvenster te maken en stel de titel in op \"Object detection\".\n",
    "   - Stel de grootte van het venster in op 400x60 pixels.\n",
    "\n",
    "2. **Maak knoppen aan**:\n",
    "   - Maak drie knoppen aan: \"Label new video\", \"Retrain model\", en \"Detect faces from livestream\".\n",
    "   - Koppel elke knop aan de bijbehorende functie (`button1_action`, `button2_action`, `button3_action`).\n",
    "   - Plaats de knoppen in een gridlayout met `grid(column, row, pady, padx)`.\n",
    "\n",
    "3. **Start de hoofdloop van de applicatie**:\n",
    "   - Gebruik `root.mainloop()` om de GUI te starten en de applicatie in een continue lus te houden totdat deze wordt afgesloten door de gebruiker.\n",
    "\n",
    "### Overzicht\n",
    "\n",
    "Deze code maakt een eenvoudige GUI voor het uitvoeren van verschillende taken gerelateerd aan objectdetectie. Gebruikers kunnen een nieuwe video labelen, het model hertrainen, of gezichten detecteren vanuit een livestream door op de bijbehorende knoppen te drukken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### App beschrijving:\n",
    "We hebben ervoor gekozen om de layout van de app zo simpel mogelijk te houden, daarom hebben wij drie knoppen:\n",
    "\n",
    "- \"Label new video\"\n",
    "- \"Retrain model\"\n",
    "- \"Detect faces from livestream\"\n",
    "\n",
    "Hier leggen wij uit wat elke knop doet:\n",
    "\n",
    "\"label new video\": Als eerst kies je jouw video uit die je wilt labelen, deze wordt vervolgens geresized en kun je beginnen met labelen door boxes te tekenen. Als je naar het volgende frame wilt gaan klik je op 'q' en als je klaar bent klik je op 'escape'. Nadat je klaar bent worden de images en labels opgeslagen in het mapje \"new_data\", hier kan je ook controleren of alle labeling goed is gegaan aangezien de afbeeldingen met bounding boxes ook opgeslagen worden. Deze map is tijdelijk en zodra de \"Retrain model\" knop gedrukt wordt, worden de images en labels verplaatst en de images met bounding boxes worden verwijderd. Dit wordt gedaan zodat je het model telkens opnieuw kan trainen met nieuwe data.\n",
    "\n",
    "\"Retrain model\": Als je op retrain drukt wordt jouw gelabelde data gebruikt uit de map \"new_data\" en verplaatst naar \"Training_runs\". Daar komt een mapje genaamd training_run_{nummer} in waar de images en labels in yolo formaat worden neergezet, zodat dit gebruikt kan worden voor het trainen. Ook wordt er een .yaml bestand gemaakt die het model verteld waar de data staat om zo opnieuw te kunnen trainen. Dan wordt uiteindelijk het model getrained en komt er een mapje genaamd training_run_{nummer}_training met daarin de resultaten van de getrainde epochs, de weigths van de laatste epoch en de beste epoch en nog een args.yaml met de argumenten die het model heeft gebruikt tijdens het trainen.\n",
    "\n",
    "\"Detect faces from livestream\": Hiervoor wordt het model gerund, als je nog niks hebt getrained dan word het door ons getrainde model gebruikt. Als je wel zelf hebt getrained wordt de laatste training run gebruikt. De livestream start en laat de boxes met labels zien waar de gezichten worden gedetecteerd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def button1_action():\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        filetypes=[(\"Image Files\", \"*.mp4\")]\n",
    "    )\n",
    "    new_path = resize_video(file_path)\n",
    "    imagePainter(new_path, 0)\n",
    "    csv_to_labels()\n",
    "    \n",
    "def button2_action():\n",
    "    create_training_run()\n",
    "    train_model()\n",
    "\n",
    "def button3_action():\n",
    "    show_detection()\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Object detection\")\n",
    "root.geometry('400x60')\n",
    "\n",
    "\n",
    "# Create buttons\n",
    "button1 = tk.Button(root, text=\"Label new video\", command=button1_action)\n",
    "button1.grid(column=0, row=1, pady=15, padx=10)\n",
    "\n",
    "button2 = tk.Button(root, text=\"Retrain model\", command=button2_action)\n",
    "button2.grid(column=1, row=1, pady=15, padx=10)\n",
    "\n",
    "button3 = tk.Button(root, text=\"Detect faces from livestream\", command=button3_action)\n",
    "button3.grid(column=2, row=1, pady=15, padx=10)\n",
    "\n",
    "# Run the application\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Duidelijk overzicht en omschrijving van het product.\n",
    "Onze AI-app maakt gebruik van een fine-tuned YOLOv8 model wat een state-of-the-art objectdetectie-model is om zo live objectdetectie(faces) te voorspellen op het apparaat waar het gerunt word. In deze app kun je zelf labelen en hertrainen van je model! Deze app zorgt ervoor dat iedereen in staat is om simpel en goed gezichten te detecteren!\n",
    "\n",
    "Onze app maakt gebruik van een voorgetrained deep learning model waardoor beginners en experts:\n",
    "Zelf data kunnen labelen, voor bijvoorbeeld eigen datasets of als je het model simpelweg nog beter wilt maken. Dit zorgt ervoor dat je alle controle hebt over je traingegevens en zo bepaal ook jij je nauwkeurigheid van het model. De app kan live gezichten detecteren door je eigen camera direct op je apparaat. Dit is erg handig als je snel en goede resultaten nodig hebt. \n",
    "\n",
    "Onze app is dus erg flexibel en geeft de gebruiker volledige controle van het labelen, hertrainen en het uiteindelijke voorspellen!\n",
    "Zo is de app te gebruiken voor real-time detectie in cameras, ook bij autonome drones of beveiliging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lijst van de stakeholders\n",
    "De app is bedoeld als oplossing voor stationaire camerabewaking daardoor zijn de stakeholders als volgt:\n",
    "- de gebruikers van de app\n",
    "- de ontwikkelaars van de app(wij)\n",
    "- iedereen die mogelijk in de gebieden komt waar de app gebruikt wordt voor camera bewaking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Datavereisten gedetailleerd beschreven\n",
    "\n",
    "De datavereisten van de app zijn als volgt:\n",
    "\n",
    "- De data moet het formaat 720x1280 zijn, dit is het formaat waarop het model wordt getraind, echter kan de app zelf videos naar dit formaat resizen, maar dit kan wel zorgen voor distorties.\n",
    "- Er moeten gezichten voorkomen in de data\n",
    "- De data moet lijken op de omstandigheden waar je wilt gaan bewaken. \n",
    "- De data moet gevarieerd zijn met belichting, afstand van de gezichten, hoeveelheid gezichten en soort gezichten\n",
    "\n",
    "Deze vereisten zijn belangrijk om te zorgen dat de training van het model daadwerkelijk verbetering oplevert. Als je namelijk een hoop videos gebruikt die op elkaar lijken om te labelen dan kan het zijn dat het model andere situaties niet goed kan gebruiken voor het detecteren van gezichten. Maar omdat het ook lastig is om overal aan te denken en goede data te verzamelen is het enorm makkelijk gemaakt om het model opnieuw te trainen, het model neemt dan de oude training runs mee, maar wordt verbeterd met de nieuwe data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Modelvereisten gedetailleerd beschreven\n",
    "\n",
    "Voor het model zelf zijn de eisen als volgt:\n",
    "\n",
    "- de output moet betrouwbaar zijn, dus weinig foute positieven, dit omdat het de waarde van een daadwerkelijke predictie omlaag haalt. als een model iets mist kan je altijd zorgen dat de livestream opgeslagen wordt en deze terugkijken. Omdat mensen erg gevoelig zijn voor gezichtsherkenning is dit een makkelijke taak als er detecties missen, echter wordt het lastiger als er overal boxes worden getekent waar geen gezichten te zien zijn.\n",
    "- de snelheid van het model moet onder de 500ms blijven, dit klinkt lang maar bewaking gaat voor lange periodes door en een lage framerate is niet erg aangezien mensen vaak langer dan een halve seconde in frame blijven. echter moet het niet langer duren dan 500ms ander is de kans te groot dat er toch detecties gemist worden.\n",
    "- het model moet in kleur werken anders worden slecht belichtte omstandigheden te moeilijk om te verwerken\n",
    "- het model moet kunnen runnen met minimale hardware, een cpu van een gemiddelde laptop zou hiervoor genoeg moeten zijn. Hierdoor wordt de app laagdrempelig en makkelijker in gebruik\n",
    "\n",
    "Deze vereisten moeten ervoor waken dat het model goed blijft werken in diverse omstandigheden. Ook zorgen deze vereisten voor een model dat betrouwbaar is en blijft voldoen aan de verwachtingen van de gebruiker. Deze vereisten hebben wij toegepast op het voorgetrainde model, maar kunnen flink verbeterd worden als de gebruiker daarvoor kiest en het model hertraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Monitoring- en hertrainingsstrategieën gedetailleerd beschreven\n",
    "\n",
    "Monitoringsstrategie: Als gebruikers merken dat het model niet goed werkt of je wilt nieuwe data toevoegen. Je kunt de prestaties van het model bekijken en als daar uitkomt dat het model niet goed presteert bieden wij met onze app een handige hertrainingsoptie waarbij je simpel een eigen video in kunt laden en opnieuw kan hertrainen.\n",
    "\n",
    "Hertrainingsstrategie:\n",
    "Onze app zorgt voor een handige hertrianingsfunctie, waarmee consumenten erg makkelijk nieuwe data kunnen labelen. Met deze nieuwe data kun je uiteindelijk ook het model trainen zonder dat je uit de app gaat en de code in moet duiken. De \"label new video\" geeft de optie om eigen video's de labelen met locaties van gezichten. Deze knop maakt gebruik van een fijne interface voor consumenten om zo simpel eigen data te labelen met bounding boxes. Nadat je de data hebt gelabeld is er een \"retrain model\" knop die het model opnieuw laat trainen met de nieuwe data hier hoef je verder niks voor te doen!\n",
    "\n",
    "Door deze hetrainingsstrategie wordt het model constant verbeterd aan nieuwe data waardoor het erg nauwkeurig blijft!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Uitleg datapipeline\n",
    "\n",
    "De data pipeline werkt als volgt:\n",
    "- de gebruiker klikt de knop \"label new video\"\n",
    "- de video wordt geresized naar 720x1280 en opgeslagen in \"new_data/resized_videos\"\n",
    "- de eerste frame van de video wordt getoond en de gebruiker kan doormiddel van de muis boxes tekenen\n",
    "- als alle gezichten gelabeld zijn drukt de gebruiker op 'q' en wordt de volgende frame getoond, dit gaat zolang door als de gebruiker wilt of als er frames in de video zijn\n",
    "- als de gebruiker klaar is drukt hij op 'escape', dit sluit de labeler af en slaat alles op\n",
    "- in de achtergrond worden de frames, de frames met de getekende boxes, alle labels (.txt bestanden) en een csv met alle labels opgeslagen in verschillende mappen in \"new_data\"\n",
    "- zodra de gebruiker genoeg videos heeft gelabeld en tevreden is drukt hij op \"retrain model\" en worden de frames(zonder getekende boxes) en labels (txt bestanden) verplaatst naar Training_runs folder. hier wordt het in een mapje gezet genaamd \"Training_run_(nummer)\" met als nummer de hoeveelste keer dat er getraint wordt. De images en labels worden in een formaat gezet dat bruikbaar is voor Yolo, dit houdt in een train en val folder met daarin de 2 folders, images en labels. in de train staan alle training data en in de val staat alle validation data. Ook wordt er een .yaml bestand aangemaakt dat ervoor zorgt dat het model kan zien waar deze folder zich bevinden en welke labels er getraind gaan worden.\n",
    "\n",
    "De data pipeline is ingericht op een manier dat er zoveel als nodig gelabeld kan worden voordat de daadwerkelijke stap naar trainen wordt gezet. Dit is handig als er nog data verzameld moet worden, maar het labelen wel al gedeeltelijk gedaan kan worden. Ook slaan we de frames met getekende boxes op om te zien of het labelings-proces goed is uitgevoerd. verder hebben we een csv bestand met alle labels, dit maakt het makkelijk om als dat nodig is de labels makkelijk te exporteren als dat nodig is.\n",
    "\n",
    "Dit opslaan vinden wij belangrijk voor de monitoring van het proces, hierdoor kan je goed in de gaten houden wat er gebeurd. echter vinden wij ook dat het overbodig is om alles oneindig te behouden, daarom worden de onnodige data verwijderd zodra er daadwerkelijk getraind wordt. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Uitleg ML-pipeline\n",
    "\n",
    "Ons trainingsproces bevindt zich in het bestand train.py.\n",
    "In dit bestand wordt het model met de nieuwste data (uit data.py) getraind. Het gebruikt de beste gewichten die eerder zijn opgeslagen en past de nieuwe data toe om zo het model nog beter te maken. \n",
    "Het werkt als volgt: \n",
    "Het model dat als laatsts getraind is wordt geselecteerd en kijkt naar de laatst gegenereerde .yaml file, hier ziet hij waar hij de images en labels kan vinden. Met de images en labels gaat het model aan de slag en wordt er getraind op 50 epochs. Het model slaat vervolgens de laatste epoch en de beste epoch op in het mapje van de training run. Ook worden de arguments die gebruikt zijn bij het trainen opgeslagen om mogelijke problemen te kunnen onderzoeken. En als laatst wordt er een csv gegenereerd met de resultaten van de verschillende epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Uitleg Deployment\n",
    "\n",
    "Zoals eerder vermeld wordt de deployment gedaan door een app, deze app is makkelijk in gebruik en heeft alles wat je nodig hebt om aan de slag te gaan met gezichten herkennen. Door alle stappen van het labelen, trainen en detecteren in de app te verwerken kunnen wij een oplossing bieden voor snel veranderende data of doeleindes die specifieke data nodig heeft. En dit zonder enorm veel verschillende applicaties, programmas, of programmeurs te gebruiken. alles in 1 simpele oplossing. De app is makkelijk te openen door app.py te runnen, hierdoor is het simpel in gebruik. \n",
    "\n",
    "De app zelf heeft duidelijk gelabelde knoppen die precies doen wat je ervan verwacht namelijk:\n",
    "\"label new video\", om een nieuwe video te labelen.\n",
    "\"retrain model\", om het model opnieuw te trainen.\n",
    "\"Detect faces from livestream\", om een livestream te openen met je webcam die vervolgens het laatste model gebruikt om gezichten te kunnen herkennen\n",
    "\n",
    "De app is simpel en zorgt dus voor een laagdrempelige instap en een makkelijke ervaring zonder afleidingen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referentielijst\n",
    "Computer vision zone. (2023, May 2). Train Yolov8 object detection on a custom dataset | Computer vision tutorial. YouTube. https://www.youtube.com/watch?v=m9fH9OWn8YM&t=3070s\n",
    "\n",
    "PyTorch. (n.d.). Get started locally. https://pytorch.org/get-started/locally/\n",
    "\n",
    "Ultralytics. (n.d.). Multi-GPU training. https://docs.ultralytics.com/nl/modes/train/#multi-gpu-training\n",
    "\n",
    "Dalvi, T. (2019, November 14). Object detection with YOLO and OpenCV: A practical guide. Medium. https://medium.com/@tejasdalvi927/object-detection-with-yolo-and-opencv-a-practical-guide-cf7773481d11\n",
    "\n",
    "Rosebrock, A. (2016, November 7). Intersection over Union (IoU) for object detection. PyImageSearch. https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n",
    "\n",
    "Vovkos. (n.d.). OpenCV showcase. https://vovkos.github.io/doxyrest-showcase/opencv/sphinx_rtd_theme/index.html\n",
    "\n",
    "ChatGPT. (2024). Prompt: |how to change x1, x2, y1, y2 to x, y, w, h format| Share link. https://chatgpt.com/share/2b0e0c97-7663-4b40-859d-1dafcae32962\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
